{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL, Database operations, and analysis\n",
    "In this notebook we will explore SQL and database operations like 'join'. Then we will use the datasets as input to a simple analysis.\n",
    "\n",
    "## Key Takeaway\n",
    "**Spark dataframes are different** \n",
    "\n",
    "## In this Noteboook\n",
    "1. create a context\n",
    "2. read in data\n",
    "3. register table with SQL\n",
    "4. perform SQL queries\n",
    "5. perform database operations\n",
    "6. do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the spark environment (takes ~ 1min)\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAppName('odl').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlc = pyspark.sql.SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important part of today's class\n",
    "\n",
    "here's where the documentation is: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPaths = [\"bryce.csv\",\"javy.csv\",\"andre.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-81fc129e89e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfbryce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfjavyb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdfandre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqlc' is not defined"
     ]
    }
   ],
   "source": [
    "dfbryce = sqlc.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(dataPaths[0])\n",
    "dfjavyb = sqlc.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(dataPaths[1])\n",
    "dfandre = sqlc.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\", \"true\").load(dataPaths[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tm: string (nullable = true)\n",
      " |-- Lg: string (nullable = true)\n",
      " |-- G: integer (nullable = true)\n",
      " |-- PA: integer (nullable = true)\n",
      " |-- AB: integer (nullable = true)\n",
      " |-- R: integer (nullable = true)\n",
      " |-- H: integer (nullable = true)\n",
      " |-- 2B: integer (nullable = true)\n",
      " |-- 3B: integer (nullable = true)\n",
      " |-- HR: integer (nullable = true)\n",
      " |-- RBI: integer (nullable = true)\n",
      " |-- SB: integer (nullable = true)\n",
      " |-- CS: integer (nullable = true)\n",
      " |-- BB: integer (nullable = true)\n",
      " |-- SO: integer (nullable = true)\n",
      " |-- BA: double (nullable = true)\n",
      " |-- OBP: double (nullable = true)\n",
      " |-- SLG: double (nullable = true)\n",
      " |-- OPS: double (nullable = true)\n",
      " |-- OPS+: integer (nullable = true)\n",
      " |-- TB: integer (nullable = true)\n",
      " |-- GDP: integer (nullable = true)\n",
      " |-- HBP: integer (nullable = true)\n",
      " |-- SH: integer (nullable = true)\n",
      " |-- SF: integer (nullable = true)\n",
      " |-- IBB: integer (nullable = true)\n",
      " |-- Pos: string (nullable = true)\n",
      " |-- Awards: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfbryce.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use some SQL\n",
    "We are going to bring in some SQL for this example\n",
    "\n",
    "## Register Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbryce.registerTempTable(\"bryce\")\n",
    "dfjavyb.registerTempTable(\"javyb\")\n",
    "dfandre.registerTempTable(\"andre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL queries to produce dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bhits = sqlc.sql(\"\"\" SELECT Year,H,AB FROM bryce \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframes\n",
    "\n",
    "Spark allows two distinct kinds of operations by the user.\n",
    "\n",
    "### Transformations\n",
    "Transformations are operations that will not be completed at the time you write and execute the code in a cell - they will only get executed once you have called a action. An example of a transformation might be to convert an integer into a float or to filter a set of values.\n",
    "\n",
    "### Actions\n",
    "Actions are commands that are computed by Spark right at the time of their execution. They consist of running all of the previous transformations in order to get back an actual result. An action is composed of one or more jobs which consists of tasks that will be executed by the workers in parallel where possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](trans_and_actions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|Year|  H| AB|\n",
      "+----+---+---+\n",
      "|2012|144|533|\n",
      "|2013|116|424|\n",
      "|2014| 96|352|\n",
      "|2015|172|521|\n",
      "|2016|123|506|\n",
      "|2017|134|420|\n",
      "|2018|137|550|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bhits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['Year, 'H, 'AB]\n",
      "+- 'UnresolvedRelation `bryce`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Year: int, H: int, AB: int\n",
      "Project [Year#12, H#20, AB#18]\n",
      "+- SubqueryAlias bryce\n",
      "   +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [Year#12, H#20, AB#18]\n",
      "+- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [Year#12, H#20, AB#18]\n",
      "+- *FileScan csv [Year#12,AB#18,H#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ec2-user/SageMaker/Spark19SpDS6003-001/bbref/bryce.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Year:int,AB:int,H:int>\n"
     ]
    }
   ],
   "source": [
    "Bhits.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](query-plan-generation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Year|  H|\n",
      "+----+---+\n",
      "|2015|172|\n",
      "+----+---+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project ['Year, 'H]\n",
      "+- 'Filter ('YEAR = 2015)\n",
      "   +- 'UnresolvedRelation `bryce`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Year: int, H: int\n",
      "Project [Year#12, H#20]\n",
      "+- Filter (YEAR#12 = 2015)\n",
      "   +- SubqueryAlias bryce\n",
      "      +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [Year#12, H#20]\n",
      "+- Filter (isnotnull(YEAR#12) && (YEAR#12 = 2015))\n",
      "   +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [Year#12, H#20]\n",
      "+- *Filter (isnotnull(YEAR#12) && (YEAR#12 = 2015))\n",
      "   +- *FileScan csv [Year#12,H#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ec2-user/SageMaker/Spark19SpDS6003-001/bbref/bryce.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Year), EqualTo(Year,2015)], ReadSchema: struct<Year:int,H:int>\n"
     ]
    }
   ],
   "source": [
    "sqlc.sql(\"\"\" SELECT Year,H FROM bryce WHERE YEAR = 2015 \"\"\").show()\n",
    "sqlc.sql(\"\"\" SELECT Year,H FROM bryce WHERE YEAR = 2015 \"\"\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|Year|  H| AB|\n",
      "+----+---+---+\n",
      "|2014| 36|213|\n",
      "|2015| 22| 76|\n",
      "|2016|115|421|\n",
      "|2017|128|469|\n",
      "|2018|176|606|\n",
      "+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Jhits = sqlc.sql(\"\"\" SELECT Year,H,AB FROM javyb \"\"\")\n",
    "Jhits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "|Year|  H| AB|\n",
      "+----+---+---+\n",
      "|1976| 20| 85|\n",
      "|1977|148|525|\n",
      "|1978|154|609|\n",
      "|1979|176|639|\n",
      "|1980|178|577|\n",
      "|1981|119|394|\n",
      "|1982|183|608|\n",
      "|1983|189|633|\n",
      "|1984|132|533|\n",
      "|1985|135|529|\n",
      "|1986|141|496|\n",
      "|1987|178|621|\n",
      "|1988|179|591|\n",
      "|1989|105|416|\n",
      "|1990|164|529|\n",
      "|1991|153|563|\n",
      "|1992|150|542|\n",
      "|1993|126|461|\n",
      "|1994| 70|292|\n",
      "|1995| 58|226|\n",
      "+----+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ahits = sqlc.sql(\"\"\" SELECT Year,H,AB FROM ANDRE \"\"\")\n",
    "Ahits.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Operations: joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Year: int, H: int, AB: int, Year: int, H: int, AB: int, Year: int, H: int, AB: int]\n",
      "== Parsed Logical Plan ==\n",
      "Join FullOuter, (Year#12 = Year#158)\n",
      ":- Join Inner, (Year#12 = Year#85)\n",
      ":  :- Project [Year#12, H#20, AB#18]\n",
      ":  :  +- SubqueryAlias bryce\n",
      ":  :     +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      ":  +- Project [Year#85, H#93, AB#91]\n",
      ":     +- SubqueryAlias javyb\n",
      ":        +- Relation[Year#85,Age#86,Tm#87,Lg#88,G#89,PA#90,AB#91,R#92,H#93,2B#94,3B#95,HR#96,RBI#97,SB#98,CS#99,BB#100,SO#101,BA#102,OBP#103,SLG#104,OPS#105,OPS+#106,TB#107,GDP#108,... 6 more fields] csv\n",
      "+- Project [Year#158, H#166, AB#164]\n",
      "   +- SubqueryAlias andre\n",
      "      +- Relation[Year#158,Age#159,Tm#160,Lg#161,G#162,PA#163,AB#164,R#165,H#166,2B#167,3B#168,HR#169,RBI#170,SB#171,CS#172,BB#173,SO#174,BA#175,OBP#176,SLG#177,OPS#178,OPS+#179,TB#180,GDP#181,... 6 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Year: int, H: int, AB: int, Year: int, H: int, AB: int, Year: int, H: int, AB: int\n",
      "Join FullOuter, (Year#12 = Year#158)\n",
      ":- Join Inner, (Year#12 = Year#85)\n",
      ":  :- Project [Year#12, H#20, AB#18]\n",
      ":  :  +- SubqueryAlias bryce\n",
      ":  :     +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      ":  +- Project [Year#85, H#93, AB#91]\n",
      ":     +- SubqueryAlias javyb\n",
      ":        +- Relation[Year#85,Age#86,Tm#87,Lg#88,G#89,PA#90,AB#91,R#92,H#93,2B#94,3B#95,HR#96,RBI#97,SB#98,CS#99,BB#100,SO#101,BA#102,OBP#103,SLG#104,OPS#105,OPS+#106,TB#107,GDP#108,... 6 more fields] csv\n",
      "+- Project [Year#158, H#166, AB#164]\n",
      "   +- SubqueryAlias andre\n",
      "      +- Relation[Year#158,Age#159,Tm#160,Lg#161,G#162,PA#163,AB#164,R#165,H#166,2B#167,3B#168,HR#169,RBI#170,SB#171,CS#172,BB#173,SO#174,BA#175,OBP#176,SLG#177,OPS#178,OPS+#179,TB#180,GDP#181,... 6 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join FullOuter, (Year#12 = Year#158)\n",
      ":- Join Inner, (Year#12 = Year#85)\n",
      ":  :- Project [Year#12, H#20, AB#18]\n",
      ":  :  +- Filter isnotnull(Year#12)\n",
      ":  :     +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      ":  +- Project [Year#85, H#93, AB#91]\n",
      ":     +- Filter isnotnull(Year#85)\n",
      ":        +- Relation[Year#85,Age#86,Tm#87,Lg#88,G#89,PA#90,AB#91,R#92,H#93,2B#94,3B#95,HR#96,RBI#97,SB#98,CS#99,BB#100,SO#101,BA#102,OBP#103,SLG#104,OPS#105,OPS+#106,TB#107,GDP#108,... 6 more fields] csv\n",
      "+- Project [Year#158, H#166, AB#164]\n",
      "   +- Relation[Year#158,Age#159,Tm#160,Lg#161,G#162,PA#163,AB#164,R#165,H#166,2B#167,3B#168,HR#169,RBI#170,SB#171,CS#172,BB#173,SO#174,BA#175,OBP#176,SLG#177,OPS#178,OPS+#179,TB#180,GDP#181,... 6 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "SortMergeJoin [Year#12], [Year#158], FullOuter\n",
      ":- *Sort [Year#12 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(Year#12, 200)\n",
      ":     +- *BroadcastHashJoin [Year#12], [Year#85], Inner, BuildRight\n",
      ":        :- *Project [Year#12, H#20, AB#18]\n",
      ":        :  +- *Filter isnotnull(Year#12)\n",
      ":        :     +- *FileScan csv [Year#12,AB#18,H#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ec2-user/SageMaker/Spark19SpDS6003-001/bbref/bryce.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Year)], ReadSchema: struct<Year:int,AB:int,H:int>\n",
      ":        +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)))\n",
      ":           +- *Project [Year#85, H#93, AB#91]\n",
      ":              +- *Filter isnotnull(Year#85)\n",
      ":                 +- *FileScan csv [Year#85,AB#91,H#93] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ec2-user/SageMaker/Spark19SpDS6003-001/bbref/javy.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Year)], ReadSchema: struct<Year:int,AB:int,H:int>\n",
      "+- *Sort [Year#158 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(Year#158, 200)\n",
      "      +- *Project [Year#158, H#166, AB#164]\n",
      "         +- *FileScan csv [Year#158,AB#164,H#166] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ec2-user/SageMaker/Spark19SpDS6003-001/bbref/andre.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Year:int,AB:int,H:int>\n"
     ]
    }
   ],
   "source": [
    "df = Bhits.join(Jhits,Bhits.Year==Jhits.Year).join(Ahits,Bhits.Year==Ahits.Year,'outer')\n",
    "print(df)\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+\n",
      "|Year|   H|  AB|Year|   H|  AB|Year|   H|  AB|\n",
      "+----+----+----+----+----+----+----+----+----+\n",
      "|null|null|null|null|null|null|1990| 164| 529|\n",
      "|null|null|null|null|null|null|1977| 148| 525|\n",
      "|2018| 137| 550|2018| 176| 606|null|null|null|\n",
      "|2015| 172| 521|2015|  22|  76|null|null|null|\n",
      "|null|null|null|null|null|null|1978| 154| 609|\n",
      "|null|null|null|null|null|null|1988| 179| 591|\n",
      "|null|null|null|null|null|null|1994|  70| 292|\n",
      "|2014|  96| 352|2014|  36| 213|null|null|null|\n",
      "|null|null|null|null|null|null|1979| 176| 639|\n",
      "|null|null|null|null|null|null|1991| 153| 563|\n",
      "|null|null|null|null|null|null|1982| 183| 608|\n",
      "|null|null|null|null|null|null|1989| 105| 416|\n",
      "|null|null|null|null|null|null|1996|  16|  58|\n",
      "|null|null|null|null|null|null|1985| 135| 529|\n",
      "|null|null|null|null|null|null|1987| 178| 621|\n",
      "|2016| 123| 506|2016| 115| 421|null|null|null|\n",
      "|null|null|null|null|null|null|1995|  58| 226|\n",
      "|null|null|null|null|null|null|1980| 178| 577|\n",
      "|null|null|null|null|null|null|1992| 150| 542|\n",
      "|null|null|null|null|null|null|1983| 189| 633|\n",
      "+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Year: int, H: int, AB: int, Year: int, H: int, AB: int]\n",
      "+----+---+---+----+---+---+\n",
      "|Year|  H| AB|Year|  H| AB|\n",
      "+----+---+---+----+---+---+\n",
      "|2014| 96|352|2014| 36|213|\n",
      "|2015|172|521|2015| 22| 76|\n",
      "|2016|123|506|2016|115|421|\n",
      "|2017|134|420|2017|128|469|\n",
      "|2018|137|550|2018|176|606|\n",
      "+----+---+---+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = Bhits.join(Jhits,Bhits.Year==Jhits.Year)\n",
    "print(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: aggregate, calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = Bhits.groupBy().agg(sf.sum('H').alias('CH'))\n",
    "CAB = Bhits.groupBy().agg(sf.sum('AB').alias('CAB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate [sum('H) AS CH#384]\n",
      "+- Project [Year#12, H#20, AB#18]\n",
      "   +- SubqueryAlias bryce\n",
      "      +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "CH: bigint\n",
      "Aggregate [sum(cast(H#20 as bigint)) AS CH#384L]\n",
      "+- Project [Year#12, H#20, AB#18]\n",
      "   +- SubqueryAlias bryce\n",
      "      +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [sum(cast(H#20 as bigint)) AS CH#384L]\n",
      "+- Project [H#20]\n",
      "   +- Relation[Year#12,Age#13,Tm#14,Lg#15,G#16,PA#17,AB#18,R#19,H#20,2B#21,3B#22,HR#23,RBI#24,SB#25,CS#26,BB#27,SO#28,BA#29,OBP#30,SLG#31,OPS#32,OPS+#33,TB#34,GDP#35,... 6 more fields] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[], functions=[sum(cast(H#20 as bigint))], output=[CH#384L])\n",
      "+- Exchange SinglePartition\n",
      "   +- *HashAggregate(keys=[], functions=[partial_sum(cast(H#20 as bigint))], output=[sum#396L])\n",
      "      +- *Project [H#20]\n",
      "         +- *FileScan csv [H#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/ec2-user/SageMaker/Spark19SpDS6003-001/bbref/bryce.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<H:int>\n"
     ]
    }
   ],
   "source": [
    "CH.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| CH|\n",
      "+---+\n",
      "|922|\n",
      "+---+\n",
      "\n",
      "+----+\n",
      "| CAB|\n",
      "+----+\n",
      "|3306|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CH.show()\n",
    "CAB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278886872353297\n"
     ]
    }
   ],
   "source": [
    "rawAVG = CH.take(1)[0][0]/CAB.take(1)[0][0]\n",
    "print(rawAVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.279"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(rawAVG,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bryce Harper career batting average: 0.279"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
