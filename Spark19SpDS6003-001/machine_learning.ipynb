{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib - machine learning in spark\n",
    "We are going to use the [pyspark api](https://spark.apache.org/docs/2.3.1/quick-start.html), and machine learning through MLlib\n",
    "\n",
    "* pyspark - https://spark.apache.org/docs/latest/api/python/index.html\n",
    "* MLlib - https://spark.apache.org/docs/latest/ml-guide.html\n",
    "* S3 (w/boto) - https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "\n",
    "## In this Noteboook\n",
    "1. create a context\n",
    "2. work with s3 and parquet to make our data frame\n",
    "3. work with MLlib on the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "from pyspark.sql.types import LongType, StringType, StructField, StructType, BooleanType, ArrayType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the spark environment (takes ~ 1min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.95.41:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>odl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=odl>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAppName('odl').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlc = pyspark.sql.SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fd73087feb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to S3\n",
    "There are a few ways to connect to S3, we are going to use boto\n",
    "* boto3 - https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "\n",
    "### Read into spark dataframe from csv in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  beta  gamma\n",
       "0      1     2      3\n",
       "1      1     4      9\n",
       "2      1     8     27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "bucket='odl-spark19spds6003-001'\n",
    "data_key = 'sample_data/data.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.createDataFrame(pd.read_csv(data_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[alpha: bigint, beta: bigint, gamma: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write parquet to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetPath = '/home/ec2-user/SageMaker/tmp-pqt'\n",
    "df.write.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep list of files to transfer\n",
    "files = [f for f in listdir(parquetPath) if isfile(join(parquetPath, f))]\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "for f in files:\n",
    "    #print('copying {} to {}'.format(parquetPath+'/'+f,\"sample_data/\"+f))\n",
    "    s3.Bucket(bucket).upload_file(parquetPath+'/'+f, \"sample_data/pqt/\"+f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to spark dataframe from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.parquet(parquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[alpha: bigint, beta: bigint, gamma: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now to start our ML pipeline\n",
    "In class we will use a sample dataset that is small so things go quickly. As an exercise for the student you can connect to a different dataset. Please load your dataset into the S3 bucket 'odl-spark19spds6003-001' and put it in a 'folder' named by your UVA NetID.\n",
    "\n",
    "## Student Exercise - Loading Data from S3 directly to dataframe\n",
    "As part of the homework you need to set up a spark dataframe for your large dataset. I did not want to deprive you of the joy of figuring out how to do that. I recommend google. Some good terms to include in the search are: pyspark, dataframe, s3, doc. NB: It is ok to struggle with this part. That's where the learning is. It is also ok to ask a friend for help, must make sure you understand the code.\n",
    "1. Make dataframe from csv file (s3://odl-spark19spds6003-001/checkouts-by-title-head.csv)\n",
    "  * nb: this one is 166MB, we will start small and then compare performance with larger version\n",
    "2. Make parquet file\n",
    "3. Make dataframe from parquet file\n",
    "4. Repeat with (s3://odl-spark19spds6003-001/checkouts-by-title.csv), 6.6GB file and note performance difference.\n",
    "5. Make a data frame with the data you want to use for MLLib\n",
    "\n",
    "\n",
    "\n",
    "* https://docs.aws.amazon.com/sagemaker/latest/dg/apache-spark-example1.html\n",
    "* https://github.com/aws-samples/aws-sagemaker-ml-blog-predictive-campaigns/blob/master/notebooks/Retail%2BData%2BDiscovery%2Band%2BProcessing%2Bv1.0.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLlib\n",
    "\n",
    "### Convention\n",
    "* df = spark dataframe\n",
    "* pddf = pandas dataframe\n",
    "\n",
    "### Outline\n",
    "1. make demo data frame\n",
    "2. exploratory tools\n",
    "3. feature engineering\n",
    "4. train/test\n",
    "5. vectorize (spark special sauce)\n",
    "6. train\n",
    "7. predict\n",
    "8. eval\n",
    "\n",
    "### Make a demo data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a sample dataframe\n",
    "bucket='odl-spark19spds6003-001'\n",
    "data_key = 'Batting.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "pddf = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8237269a2cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpddf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NB: this will case an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    336\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    337\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType())))\n\u001b[0;32m-> 1028\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mnfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType())))\n\u001b[0;32m-> 1028\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>"
     ]
    }
   ],
   "source": [
    "df = sqlc.createDataFrame(pddf) # NB: this will case an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf = pddf.dropna() # spark thinks NaN are strings and the rest are doubles, so dropnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.createDataFrame(pddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Basics\n",
    "* exploratory tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[playerID: string, yearID: bigint, stint: bigint, teamID: string, lgID: string, G: bigint, AB: bigint, R: bigint, H: bigint, 2B: bigint, 3B: bigint, HR: bigint, RBI: double, SB: double, CS: double, BB: bigint, SO: double, IBB: double, HBP: double, SH: double, SF: double, GIDP: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playerID: string (nullable = true)\n",
      " |-- yearID: long (nullable = true)\n",
      " |-- stint: long (nullable = true)\n",
      " |-- teamID: string (nullable = true)\n",
      " |-- lgID: string (nullable = true)\n",
      " |-- G: long (nullable = true)\n",
      " |-- AB: long (nullable = true)\n",
      " |-- R: long (nullable = true)\n",
      " |-- H: long (nullable = true)\n",
      " |-- 2B: long (nullable = true)\n",
      " |-- 3B: long (nullable = true)\n",
      " |-- HR: long (nullable = true)\n",
      " |-- RBI: double (nullable = true)\n",
      " |-- SB: double (nullable = true)\n",
      " |-- CS: double (nullable = true)\n",
      " |-- BB: long (nullable = true)\n",
      " |-- SO: double (nullable = true)\n",
      " |-- IBB: double (nullable = true)\n",
      " |-- HBP: double (nullable = true)\n",
      " |-- SH: double (nullable = true)\n",
      " |-- SF: double (nullable = true)\n",
      " |-- GIDP: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(playerID='colemjo02', yearID=1890, stint=1, teamID='PHI', lgID='NL', G=1, AB=0, R=0, H=0, 2B=0, 3B=0, HR=0, RBI=0.0, SB=0.0, CS=0.0, BB=0, SO=0.0, IBB=0.0, HBP=0.0, SH=0.0, SF=0.0, GIDP=0.0),\n",
       " Row(playerID='brynato01', yearID=1891, stint=1, teamID='BSN', lgID='NL', G=1, AB=0, R=0, H=0, 2B=0, 3B=0, HR=0, RBI=0.0, SB=0.0, CS=0.0, BB=0, SO=0.0, IBB=0.0, HBP=0.0, SH=0.0, SF=0.0, GIDP=0.0),\n",
       " Row(playerID='dunnian01', yearID=1891, stint=1, teamID='NY1', lgID='NL', G=1, AB=0, R=0, H=0, 2B=0, 3B=0, HR=0, RBI=0.0, SB=0.0, CS=0.0, BB=0, SO=0.0, IBB=0.0, HBP=0.0, SH=0.0, SF=0.0, GIDP=0.0),\n",
       " Row(playerID='sulliji01', yearID=1891, stint=1, teamID='BSN', lgID='NL', G=1, AB=0, R=0, H=0, 2B=0, 3B=0, HR=0, RBI=0.0, SB=0.0, CS=0.0, BB=0, SO=0.0, IBB=0.0, HBP=0.0, SH=0.0, SF=0.0, GIDP=0.0),\n",
       " Row(playerID='viaule01', yearID=1892, stint=1, teamID='CL4', lgID='NL', G=1, AB=0, R=0, H=0, 2B=0, 3B=0, HR=0, RBI=0.0, SB=0.0, CS=0.0, BB=0, SO=0.0, IBB=0.0, HBP=0.0, SH=0.0, SF=0.0, GIDP=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r(H,R) = 0.9739356029782406\n",
      "Pearson's r(HR,SO) = 0.8235057354838308\n",
      "Pearson's r(yearID,HR) = 0.02899663350555121\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson's r(H,R) = {}\".format(df.corr(\"H\", \"R\")))\n",
    "print(\"Pearson's r(HR,SO) = {}\".format(df.corr(\"HR\", \"SO\")))\n",
    "print(\"Pearson's r(yearID,HR) = {}\".format(df.corr(\"yearID\", \"HR\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But those first two correlations are probably due to how many games the player played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r(G,H) = 0.9179755244596672\n",
      "Pearson's r(G,R) = 0.8928026209617358\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson's r(G,H) = {}\".format(df.corr(\"G\", \"H\")))\n",
    "print(\"Pearson's r(G,R) = {}\".format(df.corr(\"G\", \"R\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got several ways to go from here. In this case we are going to compute some new features. H, R, HR, SO, per G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playerID: string (nullable = true)\n",
      " |-- yearID: long (nullable = true)\n",
      " |-- stint: long (nullable = true)\n",
      " |-- teamID: string (nullable = true)\n",
      " |-- lgID: string (nullable = true)\n",
      " |-- G: long (nullable = true)\n",
      " |-- AB: long (nullable = true)\n",
      " |-- R: long (nullable = true)\n",
      " |-- H: long (nullable = true)\n",
      " |-- 2B: long (nullable = true)\n",
      " |-- 3B: long (nullable = true)\n",
      " |-- HR: long (nullable = true)\n",
      " |-- RBI: double (nullable = true)\n",
      " |-- SB: double (nullable = true)\n",
      " |-- CS: double (nullable = true)\n",
      " |-- BB: long (nullable = true)\n",
      " |-- SO: double (nullable = true)\n",
      " |-- IBB: double (nullable = true)\n",
      " |-- HBP: double (nullable = true)\n",
      " |-- SH: double (nullable = true)\n",
      " |-- SF: double (nullable = true)\n",
      " |-- GIDP: double (nullable = true)\n",
      " |-- HpG: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('HpG', df.H / df.G)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playerID: string (nullable = true)\n",
      " |-- yearID: long (nullable = true)\n",
      " |-- stint: long (nullable = true)\n",
      " |-- teamID: string (nullable = true)\n",
      " |-- lgID: string (nullable = true)\n",
      " |-- G: long (nullable = true)\n",
      " |-- AB: long (nullable = true)\n",
      " |-- R: long (nullable = true)\n",
      " |-- H: long (nullable = true)\n",
      " |-- 2B: long (nullable = true)\n",
      " |-- 3B: long (nullable = true)\n",
      " |-- HR: long (nullable = true)\n",
      " |-- RBI: double (nullable = true)\n",
      " |-- SB: double (nullable = true)\n",
      " |-- CS: double (nullable = true)\n",
      " |-- BB: long (nullable = true)\n",
      " |-- SO: double (nullable = true)\n",
      " |-- IBB: double (nullable = true)\n",
      " |-- HBP: double (nullable = true)\n",
      " |-- SH: double (nullable = true)\n",
      " |-- SF: double (nullable = true)\n",
      " |-- GIDP: double (nullable = true)\n",
      " |-- HpG: double (nullable = true)\n",
      " |-- RpG: double (nullable = true)\n",
      " |-- HRpG: double (nullable = true)\n",
      " |-- SOpG: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('RpG', df.R / df.G)\n",
    "df = df.withColumn('HRpG', df.HR / df.G)\n",
    "df = df.withColumn('SOpG', df.SO / df.G)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's r(G,H) = 0.9179755244596672\n",
      "Pearson's r(G,HpG) = 0.7333576164062421\n",
      "\n",
      "Pearson's r(G,R) = 0.8928026209617358\n",
      "Pearson's r(G,RpG) = 0.702607341934959\n",
      "\n",
      "Pearson's r(H,R) = 0.9739356029782406\n",
      "Pearson's r(HpG,RpG) = 0.9041645921942068\n",
      "\n",
      "Pearson's r(HR,SO) = 0.8235057354838308\n",
      "Pearson's r(HRpG,SOpG) = 0.5032203403305449\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson's r(G,H) = {}\".format(df.corr(\"G\", \"H\")))\n",
    "print(\"Pearson's r(G,HpG) = {}\".format(df.corr(\"G\", \"HpG\")))\n",
    "print()\n",
    "print(\"Pearson's r(G,R) = {}\".format(df.corr(\"G\", \"R\")))\n",
    "print(\"Pearson's r(G,RpG) = {}\".format(df.corr(\"G\", \"RpG\")))\n",
    "print()\n",
    "print(\"Pearson's r(H,R) = {}\".format(df.corr(\"H\", \"R\")))\n",
    "print(\"Pearson's r(HpG,RpG) = {}\".format(df.corr(\"HpG\", \"RpG\")))\n",
    "print()\n",
    "print(\"Pearson's r(HR,SO) = {}\".format(df.corr(\"HR\", \"SO\")))\n",
    "print(\"Pearson's r(HRpG,SOpG) = {}\".format(df.corr(\"HRpG\", \"SOpG\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select two features for analysis\n",
    "* RBI = feature\n",
    "* R = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"RBI\",\"R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, ML time\n",
    "1. split data into train/test\n",
    "2. **SPARK SPECIAL SAUCE** -  pysparki.ml.linalg - vectorization\n",
    "3. Train --> Predict --> Evaluate\n",
    "\n",
    "### Make Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set N = 52830, test set N = 13345\n"
     ]
    }
   ],
   "source": [
    "# create train/test sets\n",
    "seed = 42\n",
    "(testDF, trainingDF) = df.randomSplit((0.20, 0.80), seed=seed)\n",
    "print ('training set N = {}, test set N = {}'.format(trainingDF.count(),testDF.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTORIZATION - spark special sauce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT # nb: bad form, done for pedagogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-udfs.html\n",
    "* User-Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL’s DSL for transforming Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[R: bigint, RBI: vector]\n"
     ]
    }
   ],
   "source": [
    "# make a user defined function (udf)\n",
    "sqlc.registerFunction(\"oneElementVec\", lambda d: Vectors.dense([d]), returnType=VectorUDT())\n",
    "\n",
    "# vectorize the data frames\n",
    "trainingDF = trainingDF.selectExpr(\"R\", \"oneElementVec(RBI) as RBI\")\n",
    "testDF = testDF.selectExpr(\"R\", \"oneElementVec(RBI) as RBI\")\n",
    "\n",
    "print(testDF.orderBy(testDF.R.desc()).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename to make ML engine happy\n",
    "trainingDF = trainingDF.withColumnRenamed(\"R\", \"label\").withColumnRenamed(\"RBI\", \"features\")\n",
    "testDF = testDF.withColumnRenamed(\"R\", \"label\").withColumnRenamed(\"RBI\", \"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML time for real\n",
    "1. Train\n",
    "2. Predict\n",
    "3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "\n",
    "lr = LinearRegression()\n",
    "lrModel = lr.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionModel"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lrModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to transform our test set to get predictions. It will append a prediction column to testDF in the new dataframe predictionsAndLabelsDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=146, features=DenseVector([72.0]), prediction=71.00487259904881), Row(label=146, features=DenseVector([81.0]), prediction=79.68985762483204), Row(label=143, features=DenseVector([130.0]), prediction=126.97477609854079), Row(label=142, features=DenseVector([128.0]), prediction=125.04477942614452), Row(label=140, features=DenseVector([72.0]), prediction=71.00487259904881)]\n"
     ]
    }
   ],
   "source": [
    "predictionsAndLabelsDF = lrModel.transform(testDF)\n",
    "\n",
    "print(predictionsAndLabelsDF.orderBy(predictionsAndLabelsDF.label.desc()).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name. (default: label)\n",
      "metricName: metric name in evaluation - one of:\n",
      "                       rmse - root mean squared error (default)\n",
      "                       mse - mean squared error\n",
      "                       r2 - r^2 metric\n",
      "                       mae - mean absolute error. (default: rmse)\n",
      "predictionCol: prediction column name. (default: prediction)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator()\n",
    "print(eval.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.evaluation.RegressionEvaluator"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.429655325124696"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.setMetricName(\"rmse\").evaluate(predictionsAndLabelsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773328491508267"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.setMetricName(\"r2\").evaluate(predictionsAndLabelsDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
